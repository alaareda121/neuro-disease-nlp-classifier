{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14904990,"datasetId":9536908,"databundleVersionId":15770067}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-21T03:00:53.702384Z","iopub.execute_input":"2026-02-21T03:00:53.702679Z","iopub.status.idle":"2026-02-21T03:00:54.992172Z","shell.execute_reply.started":"2026-02-21T03:00:53.702647Z","shell.execute_reply":"2026-02-21T03:00:54.991421Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/datasets/alaareda12/eng-data/lowercase_all_cleaned_output_no_additional_info (6) (4).xlsx\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\n\n# بيدور على الفايل في كل الـ input\nfor root, dirs, files in os.walk(\"/kaggle/input\"):\n    for file in files:\n        print(os.path.join(root, file))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T03:03:23.282737Z","iopub.execute_input":"2026-02-21T03:03:23.283640Z","iopub.status.idle":"2026-02-21T03:03:23.292361Z","shell.execute_reply.started":"2026-02-21T03:03:23.283609Z","shell.execute_reply":"2026-02-21T03:03:23.291511Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/datasets/alaareda12/eng-data/lowercase_all_cleaned_output_no_additional_info (6) (4).xlsx\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ======================================================\n# 1. Installing Libraries\n# ======================================================\n!pip install -q transformers datasets accelerate scikit-learn openpyxl\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport os\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding,\n    EarlyStoppingCallback\n)\nimport datasets\n\nwarnings.filterwarnings('ignore')\n\n# ======================================================\n# 2. Data Loading\n# ======================================================\nFILE_PATH = '/kaggle/input/datasets/alaareda12/eng-data/lowercase_all_cleaned_output_no_additional_info (6) (4).xlsx'\n\nprint(f\"Loading Data from: {FILE_PATH}\")\nraw_df = pd.read_excel(FILE_PATH)\n\nraw_df.dropna(subset=['Full_name', 'Symptoms', 'DonorID'], inplace=True)\nraw_df['Symptoms'] = raw_df['Symptoms'].astype(str).str.strip()\nraw_df['Full_name'] = raw_df['Full_name'].astype(str).str.strip().str.lower()\n\n# ======================================================\n# 3. Timeline Aggregation — ✅ أضفنا Type\n# ======================================================\ndef create_patient_timeline(group):\n    group = group.sort_values('Years_before_death', ascending=False)\n    first_row = group.iloc[0]\n\n    sex       = first_row['Sex']  if pd.notna(first_row['Sex'])  else \"Unknown\"\n    age       = first_row['Age']  if pd.notna(first_row['Age'])  else \"Unknown\"\n    typ       = first_row['Type'] if pd.notna(first_row['Type']) else \"Unknown\"  # ✅ جديد\n    diagnosis = first_row['Full_name']\n\n    events = []\n    for _, row in group.iterrows():\n        years = row['Years_before_death']\n        sym   = row['Symptoms']\n        if pd.notna(years):\n            events.append(f\"At {int(years)} years pre-death: {sym}\")\n        else:\n            events.append(f\"Observed: {sym}\")\n\n    # ✅ أضفنا Type في النص\n    full_text = f\"Patient ({sex}, Age {age}, Type {typ}). History: \" + \" -> \".join(events)\n    return pd.Series({'text': full_text, 'label_name': diagnosis})\n\nprint(\"Aggregating patient timelines by DonorID...\")\ndf_grouped = raw_df.groupby('DonorID').apply(create_patient_timeline).reset_index()\n\ntargets = [\"alzheimer's disease\", \"control brain\", \"parkinson's disease\",\n           \"progressive supranuclear palsy\", \"multiple system atrophy\"]\ndf_final = df_grouped[df_grouped['label_name'].isin(targets)].copy()\nprint(f\"Final Unique Patients: {len(df_final)}\")\nprint(df_final['label_name'].value_counts())\n\nle = LabelEncoder()\ndf_final['label'] = le.fit_transform(df_final['label_name'])\n\ntrain_df, test_df = train_test_split(\n    df_final, test_size=0.2, stratify=df_final['label'], random_state=0\n)\nprint(f\"Train size: {len(train_df)} | Test size: {len(test_df)}\")\n\n# ======================================================\n# 4. Tokenization\n# ======================================================\nMODEL_NAME = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ndef preprocess(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n\nds_train = datasets.Dataset.from_pandas(train_df).map(preprocess, batched=True)\nds_test  = datasets.Dataset.from_pandas(test_df).map(preprocess, batched=True)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le.classes_))\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return {\"accuracy\": accuracy_score(labels, predictions)}\n\n# ======================================================\n# 5. Training Arguments\n# ======================================================\nargs = TrainingArguments(\n    output_dir=\"/kaggle/working/timeline_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=2,\n    num_train_epochs=20,\n    fp16=True,\n    eval_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    save_total_limit=1,\n    report_to=\"none\"\n)\n\n# ======================================================\n# 6. Trainer\n# ======================================================\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=ds_train,\n    eval_dataset=ds_test,\n    processing_class=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=6)]\n)\n\nprint(\"Starting Training...\")\ntrainer.train()\n\n# ======================================================\n# 7. Final Evaluation\n# ======================================================\nprint(\"\\n===== Final Evaluation Report =====\")\npreds = trainer.predict(ds_test)\ny_pred = np.argmax(preds.predictions, axis=1)\ny_true = preds.label_ids\n\nprint(f\"Final Accuracy: {accuracy_score(y_true, y_pred)*100:.2f}%\")\nprint(classification_report(y_true, y_pred, target_names=le.classes_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T05:44:56.807287Z","iopub.execute_input":"2026-02-21T05:44:56.807662Z","iopub.status.idle":"2026-02-21T06:01:22.382582Z","shell.execute_reply.started":"2026-02-21T05:44:56.807633Z","shell.execute_reply":"2026-02-21T06:01:22.381817Z"}},"outputs":[{"name":"stdout","text":"Loading Data from: /kaggle/input/datasets/alaareda12/eng-data/lowercase_all_cleaned_output_no_additional_info (6) (4).xlsx\nAggregating patient timelines by DonorID...\nFinal Unique Patients: 1329\nlabel_name\nalzheimer's disease               703\ncontrol brain                     341\nparkinson's disease               133\nprogressive supranuclear palsy     91\nmultiple system atrophy            61\nName: count, dtype: int64\nTrain size: 1063 | Test size: 266\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1063 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c290fab310e41769464c02a4cb0ce30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/266 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f3c1a374d464d6ebe761e69b3aaafe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0963536300b6473783493abfa5bdae14"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\nKey                                        | Status     | \n-------------------------------------------+------------+-\ncls.seq_relationship.bias                  | UNEXPECTED | \ncls.predictions.transform.LayerNorm.weight | UNEXPECTED | \ncls.predictions.decoder.weight             | UNEXPECTED | \ncls.predictions.transform.dense.weight     | UNEXPECTED | \ncls.seq_relationship.weight                | UNEXPECTED | \ncls.predictions.bias                       | UNEXPECTED | \ncls.predictions.transform.dense.bias       | UNEXPECTED | \ncls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \ncls.predictions.decoder.bias               | UNEXPECTED | \nclassifier.weight                          | MISSING    | \nclassifier.bias                            | MISSING    | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Starting Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='476' max='680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [476/680 16:06 < 06:56, 0.49 it/s, Epoch 14/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.483384</td>\n      <td>0.971070</td>\n      <td>0.838346</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.638865</td>\n      <td>0.816500</td>\n      <td>0.830827</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.431054</td>\n      <td>0.721066</td>\n      <td>0.875940</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.415827</td>\n      <td>0.682151</td>\n      <td>0.909774</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.215608</td>\n      <td>0.602142</td>\n      <td>0.913534</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.976646</td>\n      <td>0.577086</td>\n      <td>0.921053</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.794073</td>\n      <td>0.595576</td>\n      <td>0.921053</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.698344</td>\n      <td>0.577579</td>\n      <td>0.932331</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.625562</td>\n      <td>0.564204</td>\n      <td>0.928571</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.563874</td>\n      <td>0.641355</td>\n      <td>0.906015</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.491572</td>\n      <td>0.666904</td>\n      <td>0.921053</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.381852</td>\n      <td>0.687789</td>\n      <td>0.909774</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.293282</td>\n      <td>0.647858</td>\n      <td>0.924812</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.245984</td>\n      <td>0.691889</td>\n      <td>0.921053</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"600e172682f243b8988c1c0097055775"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"562e5c809bc64c068c04f5ddc7efe2c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7131e9ebbaf3485893eaaa91f4c2f73f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3759f27585a48fd88a190ea9958af2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c7dd98cc1c540c5815e3fac56ef41e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65c21391a19847b6b25fde4faeb856fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a9a85200adc4b8e9a63cd51db260ce4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e814e8da9244fb6950c2a4a636d54f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a145b5eecf6847f18c638608f35159aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f072c3ac90ea4a56871c36bf61cfa285"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e76aa129c4444440ac4a02976542904a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"996eb2a905444e2ea1dfa8d64a86d3e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03e4d992f4124d3d840f6af601a49c92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e837f249062c43e09113cc8f56b44373"}},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias'].\nThere were unexpected keys in the checkpoint model loaded: ['bert.embeddings.LayerNorm.beta', 'bert.embeddings.LayerNorm.gamma', 'bert.encoder.layer.0.attention.output.LayerNorm.beta', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma', 'bert.encoder.layer.0.output.LayerNorm.beta', 'bert.encoder.layer.0.output.LayerNorm.gamma', 'bert.encoder.layer.1.attention.output.LayerNorm.beta', 'bert.encoder.layer.1.attention.output.LayerNorm.gamma', 'bert.encoder.layer.1.output.LayerNorm.beta', 'bert.encoder.layer.1.output.LayerNorm.gamma', 'bert.encoder.layer.2.attention.output.LayerNorm.beta', 'bert.encoder.layer.2.attention.output.LayerNorm.gamma', 'bert.encoder.layer.2.output.LayerNorm.beta', 'bert.encoder.layer.2.output.LayerNorm.gamma', 'bert.encoder.layer.3.attention.output.LayerNorm.beta', 'bert.encoder.layer.3.attention.output.LayerNorm.gamma', 'bert.encoder.layer.3.output.LayerNorm.beta', 'bert.encoder.layer.3.output.LayerNorm.gamma', 'bert.encoder.layer.4.attention.output.LayerNorm.beta', 'bert.encoder.layer.4.attention.output.LayerNorm.gamma', 'bert.encoder.layer.4.output.LayerNorm.beta', 'bert.encoder.layer.4.output.LayerNorm.gamma', 'bert.encoder.layer.5.attention.output.LayerNorm.beta', 'bert.encoder.layer.5.attention.output.LayerNorm.gamma', 'bert.encoder.layer.5.output.LayerNorm.beta', 'bert.encoder.layer.5.output.LayerNorm.gamma', 'bert.encoder.layer.6.attention.output.LayerNorm.beta', 'bert.encoder.layer.6.attention.output.LayerNorm.gamma', 'bert.encoder.layer.6.output.LayerNorm.beta', 'bert.encoder.layer.6.output.LayerNorm.gamma', 'bert.encoder.layer.7.attention.output.LayerNorm.beta', 'bert.encoder.layer.7.attention.output.LayerNorm.gamma', 'bert.encoder.layer.7.output.LayerNorm.beta', 'bert.encoder.layer.7.output.LayerNorm.gamma', 'bert.encoder.layer.8.attention.output.LayerNorm.beta', 'bert.encoder.layer.8.attention.output.LayerNorm.gamma', 'bert.encoder.layer.8.output.LayerNorm.beta', 'bert.encoder.layer.8.output.LayerNorm.gamma', 'bert.encoder.layer.9.attention.output.LayerNorm.beta', 'bert.encoder.layer.9.attention.output.LayerNorm.gamma', 'bert.encoder.layer.9.output.LayerNorm.beta', 'bert.encoder.layer.9.output.LayerNorm.gamma', 'bert.encoder.layer.10.attention.output.LayerNorm.beta', 'bert.encoder.layer.10.attention.output.LayerNorm.gamma', 'bert.encoder.layer.10.output.LayerNorm.beta', 'bert.encoder.layer.10.output.LayerNorm.gamma', 'bert.encoder.layer.11.attention.output.LayerNorm.beta', 'bert.encoder.layer.11.attention.output.LayerNorm.gamma', 'bert.encoder.layer.11.output.LayerNorm.beta', 'bert.encoder.layer.11.output.LayerNorm.gamma'].\n","output_type":"stream"},{"name":"stdout","text":"\n===== Final Evaluation Report =====\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Final Accuracy: 93.23%\n                                precision    recall  f1-score   support\n\n           alzheimer's disease       0.96      0.98      0.97       141\n                 control brain       1.00      1.00      1.00        68\n       multiple system atrophy       0.73      0.67      0.70        12\n           parkinson's disease       0.74      0.93      0.82        27\nprogressive supranuclear palsy       1.00      0.50      0.67        18\n\n                      accuracy                           0.93       266\n                     macro avg       0.88      0.81      0.83       266\n                  weighted avg       0.94      0.93      0.93       266\n\n","output_type":"stream"}],"execution_count":13}]}